+++
author = "Eldridge Alexander"
date = 2022-07-28T02:03:11Z
description = "The villian AI is here, but it's not what we thought it would be."
draft = false
image = "img/ai.jpg"
slug = "the-silent-ai-overlord-is-already-here"
title = "The Silent AI Overlord is Already Here"

+++

Every few months or years it seems like there’s at least a few news cycles devoted to how Artificial Intelligence is an existential threat to humanity. Sometimes it’s a general warning from Stephen Hawking, or Elon Musk founding a new company. Pop culture also makes AI into the villian in things like I, Robot; Terminator; Age of Ultron; or even in media aimed more at family and young children like WALL-E. These are existential threats that are truly dangerous but entirely theoretical. Much like an asteroid destroying life on earth, we should continue researching and preparing but should allocate resources according to the likelihood of the event occurring. We spend more money and effort on healthcare and reducing pollution than we do on preventing asteroid hits. This is because they are present, widespread, and tractable problems. An asteroid hit is certainly possible, but the possibility is slim and healthcare and pollution are currently existing concerns we can address.

This fear mongering about a potential sapient AI distracts from the very real and currently present threats from a non-sentient AI. Let me break down some of these terms as I see them used most often by experts in these spaces. AI is a bit of a generic term. You rarely hear engineers use it when addressing other engineers. They tend to use phrases like Machine Learning (ML) or Natural Language Processing (NLP). The dangerous, sapient AI of movies and comics is usually referred to as Artificial General Intelligence (AGI). This would be a single intelligence that could theoretically learn any general thought process or task the way a human could. 

AGI would have sapience and thus a consciousness (or something much like it). Consciousness is something not well understood by humanity. Psychiatrists, psychologists, and philosophers can’t even agree on what it fundamentally is. Since we cannot consistently define consciousness this makes predicting its creation or emergence difficult at best. While the resources of companies, think tanks, ethicists, and more are dedicated to digging into the issues an AGi would present to humanity, those resources are consequently not being devoted to the ethics of modern AI (MK, NLP, etc.). They are addressed but get much less attention in the news and media, as the algorithm dictating the speed of your Instagram notifications is inherently less compelling than a James Spader-voiced megalomaniac robot[^1].

[^1]: Yes, I know I’m the only person that really enjoys the Age of Ultron movie. Don’t @ me.


The premise behind the risks of modern AI is simple, even if the specifics are endlessly complicated. Modern AIs are variations on ML. ML is done by having an ML program that processes an unbelievably huge amount of data until it can make generalized conclusions. If the data contains inaccuracy, bias, or other issues at scale those issues will be reflected in the AI. Not only reflected but exacerbated. One of the benefits of ML is that it can learn unbelievably  fast. AlphaGo went from “conception” to being the best Go player in the world in a handful of years. And it took less than a week for Microsoft’s AI chatbot “Tay” to become a Nazi.

A common application of ML is facial recognition. Facebook has been having people tag their friends in photos for over a decade, and for most of that time has had users specify which face was whose. This gave them an unbelievably large data set to train their AI with. Once the AI was advanced enough (i.e. it had trained on a large enough data set) it was able to begin tagging faces automatically with a high degree of accuracy. 

Google, like many tech companies, is based in the Bay area and sources a lot of their training data accordingly. This means that the bias in their training is reflected in their products. For example the time their photo recognition algorithm classified two Black Americans as gorillas. Despite this, only a few years later Google’s recognition tech was being used by the Pentagon to analyze drone footage. (We can’t be sure it was the same software in both cases but there seems to have been no fundamental changes to how Google approached AI during that time). This example is specific to Google but is emblematic of modern tech company AIs.

AI algorithms are already and increasingly being used in various facets of life: facial recognition, location tracking, sentiment evaluation, gait analysis, and financial responsibility. They’re used by police and state surveillance organizations. They’re also used for corporate surveillance  which is often sold to governments. They’re used in hiring and retention decisions at companies, by banks when deciding to make a loan, and by landlords when making renting decisions.

Putting decision making in the hands of an AI is dangerous as they are difficult or impossible to audit. And they also allow fast scaling of a product or service that probably shouldn’t have. Companies that have error rates around 1% is a pretty big issue when their customer base measures in the hundreds of millions or billions.

You see this in practice when women get lower credit offerings, wrongful arrests happen, interview and hiring decisions are made based on AI-measure emotions. It seems like an (alleged) slide from an IBM presentation in 1979 applies here

![A computer can never be held accountable, therefore a computer must never make a management decision](img/ibm-accountable.png "San Juan Mountains")

I really like AI and think it’s a cool technology that I’m excited to see evolve but its marketing has surpassed its abilities. Attempting to remove bias from data sets, or make an AI that can identify and not internalize that bias is a huge question. Researchers are making progress but it’s early days. While I’m excited to see how this area evolves, we should be cautious. We should allow almost any kind of research, but strongly restrict applications until we have substantial testing proving their safety. Tech has a history of ignoring long tails of users, but for AIs that increasingly run society we can’t afford to be that lax.

We can address this in several ways. First and foremost, require AI data to be public. Companies don’t have to share their proprietary datasets, but should have to share enough data and information about how they train their AI so that researchers that aren’t paid by that company can do research on their data to validate how it’s being used. 

One significant consideration of AI is how it enabled new uses of data that were difficult to foresee. While even a decade ago privacy advocates were advising against facial recognition and similar, few warned against deep fakes where a short video of yourself could be used to mimic you in false situations that looked real. Whether having your face believably on a body in a porn scene, or the President making a speech that never happened, it was difficult to foresee those possibilities. As such, we should be careful about Personally Identifiable INformation (PII) going forward. Personal data should be treated like nuclear waste. We shouldn't stop iots generation as the product is (generally) worth it, but we should make sure that handling of that data is highly controlled to avoid contaminating the environment. 

Another should be a nutrition label on AI and companies that use AI to generate their products. For example, how their AI drives engagement, etc.

Any company generating a news feed via algorithm, such as Twitter, Facebook,and Youtube but also less obvious uses such as LinkedIn should be held responsible for what they publish. As I call out in my piece on free speech these companies are presently more analogous to newspaper editors than they were when our current computer laws were conceived. 

The New York Times (for example) has an editorial responsibility. They are strongly protected by the first amendment, however are liable if they publish libel, commit fraud, etc, as they made the editorial decision to publish the content. Even if the content was an op ed submitted by a non-staff member. These modern platforms are closer to that than a bulletin board.

If I post a controversial tweet, my relatively small number of followers will see it. However Twitter’s algorithm can decide to amplify that and show it to people it anticipates will engage with my tweet, who will engage and the algorithm will amplify it again. Don’t believe me? Go on twitter and see what's trending. Chances are something is being shown there with only a few thousand likes on a platform of almost half a billion people.

With almost endless content to choose from, deciding what to amplify is inherently an editorial decision and the company should be held accountable for the editorial decisions of the AI, the same as it would for a human editor.

Also, certain levels of AI driven products should be aged. The same as alcohol and cannabis.  


